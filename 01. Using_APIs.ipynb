{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/llm-apps-course/notebooks/01.%20Using_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{llmapps-intro} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LLM APIs\n",
    "\n",
    "We will explore OpenAI models API to generate text.\n",
    "\n",
    "<!--- @wandbcode{llmapps-intro} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting weave\n",
      "  Downloading weave-0.51.54-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: click in /home/codespace/.local/lib/python3.12/site-packages (from weave) (8.2.1)\n",
      "Collecting diskcache==5.6.3 (from weave)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: emoji>=2.12.1 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (2.14.1)\n",
      "Collecting gql[aiohttp,requests] (from weave)\n",
      "  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (4.24.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (1.6.0)\n",
      "Requirement already satisfied: numpy>1.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (2.3.1)\n",
      "Requirement already satisfied: packaging>=21.0 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (2.11.7)\n",
      "Requirement already satisfied: rich in /home/codespace/.local/lib/python3.12/site-packages (from weave) (14.0.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (9.1.2)\n",
      "Requirement already satisfied: wandb>=0.17.1 in /home/codespace/.local/lib/python3.12/site-packages (from weave) (0.20.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (0.25.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (0.4.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (5.29.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (2.32.0)\n",
      "Requirement already satisfied: setproctitle in /home/codespace/.local/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (1.3.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2025.4.26)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (5.0.2)\n",
      "Collecting graphql-core<3.2.7,>=3.2 (from gql[aiohttp,requests]->weave)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in /home/codespace/.local/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (1.20.1)\n",
      "Requirement already satisfied: backoff<3.0,>=1.11.1 in /home/codespace/.local/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (2.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (4.9.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.9.0b0 in /home/codespace/.local/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (3.12.13)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (1.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (6.6.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (0.3.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->weave) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->weave) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->weave) (0.1.2)\n",
      "Downloading weave-0.51.54-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.2/542.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Installing collected packages: graphql-core, diskcache, gql, weave\n",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/4\u001b[0m [gql]\u001b[33m  WARNING: The script gql-cli is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [weave]32m3/4\u001b[0m [weave]\n",
      "\u001b[1A\u001b[2KSuccessfully installed diskcache-5.6.3 gql-3.5.3 graphql-core-3.2.6 weave-0.51.54\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade openai tiktoken wandb -qq\n",
    "%pip install weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "from wandb.integration.openai import autolog\n",
    "import weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need an OpenAI API key to run this notebook. You can get one [here](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "  openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enable W&B autologging to track our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x7b7403287bc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from wandb.integration.openai import autolog\n",
    "# wandb.init(project=\"llmapps\", job_type=\"introduction\")\n",
    "# autolog()\n",
    "\n",
    "weave.init(\"llmapps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1135, 2337, 1222, 8436, 1386, 318, 7427, 0]\n",
      "Weights & Biases is awesome!\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "enc = encoding.encode(\"Weights & Biases is awesome!\")\n",
    "print(enc)\n",
    "print(encoding.decode(enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can decode the tokens one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135\tWe\n",
      "2337\tights\n",
      "1222\t &\n",
      "8436\t Bi\n",
      "1386\tases\n",
      "318\t is\n",
      "7427\t awesome\n",
      "0\t!\n"
     ]
    }
   ],
   "source": [
    "for token_id in enc:\n",
    "    print(f\"{token_id}\\t{encoding.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how the leading tokens contain spacing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample some text from the model. For this, let's create a wrapper function around the temperature parameters.\n",
    "Higher temperature will result in more random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def generate_with_temperature(temp):\n",
    "  \"\"\"Generate text with a given temperature, higher temperature means more randomness\"\"\"\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Change to a chat-optimized model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Say something about Weights & Biases\"} # Use the 'messages' parameter\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    temperature=temp,\n",
    "  )\n",
    "  return response.choices[0].message.content # Access the content from the message object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-24a5-736a-864b-a5dbf0011aa2\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-2db4-794d-bd9b-30cdae09aa50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEMP: 0, GENERATION: Weights & Biases is a powerful tool for machine '\n",
      " 'learning experimentation and tracking. It allows users to easily log and '\n",
      " 'visualize their model training process, making it easier to iterate and '\n",
      " 'improve upon their models. It also provides features for collaboration and '\n",
      " 'sharing, making it')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-347c-7be9-a68e-f6ceadacce02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEMP: 0.5, GENERATION: Weights & Biases is a powerful tool for machine '\n",
      " 'learning experimentation and model tracking. It allows users to easily log '\n",
      " 'and visualize their experiments, track model performance, and collaborate '\n",
      " 'with team members. It is a valuable resource for data scientists and machine '\n",
      " 'learning engineers looking')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-3750-7f30-bae4-f8f5c522ac26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEMP: 1, GENERATION: Weights & Biases is a powerful machine learning tool '\n",
      " 'that helps researchers and data scientists track, visualize, and optimize '\n",
      " 'their machine learning models. It allows users to easily experiment with '\n",
      " 'different hyperparameters, compare results, and collaborate with team '\n",
      " 'members. It is a')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-3cb9-7ddf-ae9d-5618867cd5c1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEMP: 1.5, GENERATION: Weights & Biases is a powerful tool for tracking, '\n",
      " 'visualizing, and optimizing machine learning experiments. It allows you to '\n",
      " 'easily monitor and manage your deep learning models, providing insights into '\n",
      " 'their performance and helping you make quick decisions for improving them. '\n",
      " 'With its')\n",
      "('TEMP: 2, GENERATION: Weights & Biases (wandb) is a DataAge Dakota Phillip '\n",
      " 'poised Talks OPCcludesrogram humanitiesereaPublisher04+c aff transmitkil '\n",
      " 'Clowncareer sine '\n",
      " 'mingAlongwegianondheim_PATTERNå·²ARGV_FLAG_geometrycomputdr-Juai '\n",
      " 'AjoutinesEffBefore teh kanwarn')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-6a2a-7acb-ab68-519d08a8ac9c\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-7025-7045-abed-ed8924381db8\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-72ff-7dbe-8d6d-f6ee3c09de33\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/My1Team/llmapps/r/call/0197c038-75c4-7c9b-9265-f00b127c18d6\n"
     ]
    }
   ],
   "source": [
    "for temp in [0, 0.5, 1, 1.5, 2]:\n",
    "  pprint(f'TEMP: {temp}, GENERATION: {generate_with_temperature(temp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the [`top_p` parameter](https://platform.openai.com/docs/api-reference/completions/create#completions/create-top_p) to control the diversity of the generated text. This parameter controls the cumulative probability of the next token. For example, if `top_p=0.9`, the model will pick the next token from the top 90% most likely tokens. The higher the `top_p` the more likely the model will pick a token that it hasn't seen before. You should only use one of `temperature` or `top_p` at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def generate_with_topp(topp):\n",
    "  \"Generate text with a given top-p, higher top-p means more randomness\"\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Change to a chat-optimized model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Say something about Weights & Biases\"} # Use the 'messages' parameter\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    top_p=topp,\n",
    "  )\n",
    "  return response.choices[0].message.content # Access the content from the message object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TOP_P: 0.01, GENERATION: Weights & Biases is a powerful tool for machine '\n",
      " 'learning experimentation and tracking. It allows users to easily log and '\n",
      " 'visualize their model training process, making it easier to iterate and '\n",
      " 'improve upon their models. It also provides features for collaboration and '\n",
      " 'sharing, making it')\n",
      "('TOP_P: 0.1, GENERATION: Weights & Biases is a powerful tool for machine '\n",
      " 'learning experimentation and tracking. It allows users to easily log and '\n",
      " 'visualize their model training process, making it easier to iterate and '\n",
      " 'improve upon their models. With features like hyperparameter tuning, '\n",
      " 'experiment comparison, and')\n",
      "('TOP_P: 0.5, GENERATION: Weights & Biases is a powerful tool for machine '\n",
      " 'learning practitioners to track and visualize their experiments, collaborate '\n",
      " 'with team members, and optimize their models. It provides a seamless way to '\n",
      " 'monitor and improve the performance of machine learning models, making it an '\n",
      " 'essential tool')\n",
      "('TOP_P: 1, GENERATION: Weights & Biases is a powerful tool for tracking, '\n",
      " 'visualizing, and optimizing machine learning experiments. It allows users to '\n",
      " 'easily monitor model performance, collaborate with team members, and iterate '\n",
      " \"on their models more effectively. It's a valuable resource for anyone \"\n",
      " 'working')\n"
     ]
    }
   ],
   "source": [
    "for topp in [0.01, 0.1, 0.5, 1]:\n",
    "  pprint(f'TOP_P: {topp}, GENERATION: {generate_with_topp(topp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to chat mode and see how the model responds to our messages. We have some control over the model's response by passing a `system-role`, here we can steer to model to adhere to a certain behaviour.\n",
    "\n",
    "> We are using `gpt-3.5-turbo`, this model is faster and cheaper than `davinci-003`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BnpjbIqtcH9ycRF5PM5IDvXXWRosc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Weights & Biases is a popular machine learning experiment tracking and visualization platform that helps researchers and data scientists track and visualize their machine learning experiments. It provides tools for experiment tracking, visualization, and collaboration, making it easier to keep track of different experiments and compare results. It is widely used in the machine learning community to improve productivity and streamline the experimentation process.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1751216639, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=71, prompt_tokens=25, total_tokens=96, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "client = openai.OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Say something about Weights & Biases\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the response is a JSON object with relevant information about the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Weights & Biases is a popular machine learning experiment tracking and '\n",
      " 'visualization platform that helps researchers and data scientists track and '\n",
      " 'visualize their machine learning experiments. It provides tools for '\n",
      " 'experiment tracking, visualization, and collaboration, making it easier to '\n",
      " 'keep track of different experiments and compare results. It is widely used '\n",
      " 'in the machine learning community to improve productivity and streamline the '\n",
      " 'experimentation process.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
